{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21bc81ab-a16f-4ab0-b6b1-b9cbf1e25ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 조절\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# os.environ[\"GRADIO_SHARE\"]=\"3\"\n",
    "# os.environ[\"WORLD_SIZE\"] = \"3\"\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"meetkai/functionary-medium-v3.2\"\n",
    "MAX_SEQ_LENGTH = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "DTYPE = None #Auto\n",
    "LOAD_IN_4BIT = True\n",
    "LORA_R = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "LORA_ALPHA = 16\n",
    "LORA_RANDOM_STATE = 148177255\n",
    "EXCEL_PATH = '~/S11P21B109/backend/fastapi/inference/data.xlsx'\n",
    "TRAINER_BATCH=2\n",
    "TRAINER_ACCUMULATION=5\n",
    "TRAINER_EPOCH=1\n",
    "TRAINER_LR = 2e-4\n",
    "SAVE_SETTING = \"merged_iq2_xxs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d548d-4191-43db-bfd9-473edfa28aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "# padding 없는 건 정상\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a0dfe-7cb4-43a8-827a-b88cef3eeb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_R, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = LORA_RANDOM_STATE,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1999ec4-8308-47af-9ff3-222d1163160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Step 1: Load the XLSX file into a pandas DataFrame\n",
    "df = pd.read_excel(EXCEL_PATH)  # Replace 'your_dataset.xlsx' with your actual file path\n",
    "\n",
    "# Step 2: Ensure the DataFrame has the required columns\n",
    "# If your columns are named differently, rename them\n",
    "df = df.rename(columns={\n",
    "    'Prompt': 'instruction',\n",
    "    'User': 'input',\n",
    "    'Assistant': 'output'\n",
    "})\n",
    "# df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16643966-864f-4ad4-86f4-ad4996b52ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to 'string' dtype\n",
    "df['instruction'] = df['instruction'].astype('string')\n",
    "df['input'] = df['input'].astype('string')\n",
    "df['output'] = df['output'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eca3f5-9a64-4e3c-9e88-7da98c174bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ced50-c410-4f98-b589-5d3e4347c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbde2c7-5d4f-4cb0-94a7-f978376b7dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Convert the DataFrame into a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7482a-37bf-48ba-b720-c9a89f0114e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = \"<|start_header_id|>system<|end_header_id|>\" + instruction + EOS_TOKEN\\\n",
    "        + \"<|start_header_id|>user<|end_header_id|>\" + input + EOS_TOKEN\\\n",
    "        + \"<|start_header_id|>assistant<|end_header_id|>\" + \"all\\n\" +output + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51561277-3f98-40d9-9e46-1d0a6b63085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a941b-d88a-471d-bf01-deae6061a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = TRAINER_BATCH,\n",
    "        gradient_accumulation_steps = TRAINER_ACCUMULATION,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = TRAINER_EPOCH, # Set this for 1 full training run.\n",
    "        learning_rate = TRAINER_LR,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = LORA_RANDOM_STATE,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d7ca6-0a83-4177-aeda-f016e12425eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601d18c-7124-49a6-8e24-05359d07ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb4b7b-bffe-4e3c-8795-0190ffa28639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장. https://github.com/unslothai/unsloth/wiki 의 Manually saving to GGUF 참조\n",
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = SAVE_SETTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d04946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama cpp 를 실행하는 conda enviroment로 실행\n",
    "# git clone --recursive https://github.com/ggerganov/llama.cpp\n",
    "# make clean -C llama.cpp\n",
    "# make all -j -C llama.cpp\n",
    "# pip install gguf protobuf\n",
    "\n",
    "# python llama.cpp/convert_hf_to_gguf.py merged_model --outfile converted --outtype QT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6fd4d7-94ae-4c28-b4d8-d282170ed284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model output check\n",
    "# # alpaca_prompt = Copied from above\n",
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "# from transformers import TextStreamer\n",
    "\n",
    "# for datadict in dataset:\n",
    "#     inputs = tokenizer(\n",
    "#         [\"<|start_header_id|>system<|end_header_id|>\" + datadict[\"instruction\"] + EOS_TOKEN\\\n",
    "#         + \"<|start_header_id|>user<|end_header_id|>\" + datadict[\"input\"] + EOS_TOKEN\\\n",
    "#         + \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "#         ], return_tensors = \"pt\").to(\"cuda\")\n",
    "#     text_streamer = TextStreamer(tokenizer)\n",
    "#     _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
